% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/measurement_error.R
\name{sleuth_fit}
\alias{sleuth_fit}
\title{Fit a measurement error model}
\usage{
sleuth_fit(obj, formula = NULL, fit_name = NULL, ...)
}
\arguments{
\item{obj}{a \code{sleuth} object}

\item{formula}{an R formula specifying the design to fit OR a design matrix.
If you are interested in only fitting the model that was specified in \code{sleuth_prep}
you do not need to specify it again (will be fit as the 'full' model).}

\item{fit_name}{the name to store the fit in the sleuth object (at so$fits$fit_name).
If \code{NULL}, the model will be named 'full'.}

\item{...}{advanced options for \code{sleuth_fit}. See details.}
}
\value{
a sleuth object with updated attributes.
}
\description{
This function is a wrapper for fitting a measurement error model using
\code{sleuth}. It performs the technical variance estimation from the boostraps, biological
variance estimation, and shrinkage estimation.
}
\details{
For most users, simply providing the sleuth object should be sufficient. By
default, this behavior will fit the full model initially specified and store
it in the sleuth object under 'full'.

To see which models have been fit, users will likely find the function
\code{\link{models}} helpful.

There are some advanced options for users how wish to customize the fitting procedure.
Note that these options have not been thoroughly tested, so their effect on the accuracy
of the results are unknown. Here are those advanced options:

Advanced options for modeling choice:

\itemize{  
  \item \code{which_var}: which kind of data (counts or TPMs) should be fit? Sleuth by
  default models the estimated counts, but can model the TPMs. This argument only accepts
  \code{'obs_counts'} (default) or \code{'obs_tpm'}. Note that if \code{gene_mode} is \code{TRUE},
  and transcript counts were aggregated to the gene-level, \code{'obs_counts'} will model
  the \code{'scaled_reads_per_base'} summary statistic.
}

Advanced options for the shrinkage procedure:

\itemize{
  \item \code{shrink_fun}: this function does the shrinkage procedure. It must take the list produced
  by \code{\link{me_model}}, with the full model and the data.frame with means (the \code{'mean_obs'}
  column) and variances (the \code{'sigma_sq_pmax'} column), as input, and must output a modified
  data.frame from list$mes_df, with the required column \code{'smooth_sigma_sq'} containing the
  smoothed variances. Additional columns are allowed, but will not be directly processed by
  \code{\link{sleuth_results}}. The default option for this option is \code{\link{basic_shrink_fun}},
  which is the procedure described in the original sleuth paper.
}

Advanced options for the default sleuth shrinkage procedure (see \code{\link{basic_shrink_fun}}):

\itemize{
  \item \code{n_bins}: the number of bins that the data should be split for the sliding window shrinkage
  using the mean-variance curve. The default is 100.
  \item \code{lwr}: the lower range of variances within each bin that should be included for the shrinkage
  procedure. The default is 0.25 (meaning the 25th percentile).
  \item \code{upr}: the upper range of variances within each bin that should be included for the shrinkage
  procedure. The default is 0.75 (meaning the 75th percentile).
}
}
\examples{
# If you specified the formula in sleuth_prep, you can simply run to run the full model
so <- sleuth_fit(so)
# The intercept only model can be fit like this
so <- sleuth_fit(so, ~1, 'reduced')
}
\seealso{
\code{\link{models}} for seeing which models have been fit,
\code{\link{sleuth_prep}} for creating a sleuth object,
\code{\link{basic_shrink_fun}} for the sleuth variance shrinkage procedure,
\code{\link{sleuth_wt}} to test whether a coefficient is zero,
\code{\link{sleuth_lrt}} to test nested models.
}
